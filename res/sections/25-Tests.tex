\chapter{Tests and metrics}

After the testbed implementation, tests about its performance and metrics were
gathered in order to understand the real project effectiveness. We performed
tests relatively two main sections: one testing the architecture overall
responsiveness and one checking the SFC implementation and efficiency.

\section{System responsiveness}

\subsection{Docker vs Virtual Box startup measurement}

Regarding the overall system responsiveness, a metric we considered worth to 
test was the VNF start up time in a docker environment versus one virtualized 
through a common virtual machine system (in this case Virtual Box was chosen). 
To make this simulation as fair as possible, we used the same node (an 
Openstack VM with 32GB of RAM, 8 vCPU and solid state storage) and used the same 
``boot sequence'' for both the VNFs: first we launched the Astaire framework, 
ensuring its availability to process data and then we notified the test 
machine of the successful boot. The startup time was measured from the 
beginning of the startup command (\verb!docker run! for Docker and 
\verb!VBoxManage -s! for Virtual Box) till the first TCP hit received by the 
test backend (to make things as smooth as possible, \verb!netcat! was used 
to listen to incoming TCP data).

It is worth saying that Docker metrics are generally more precise than the
Virtual Box one: Docker allows to inspect the container and to gain startup and
shutdown times with a precision of millseconds, while to gather this information
for VirtualBox instances we had to use the command line utility \verb!time!,
that registered timing with a precision of hundredths of a second. Nonetheless,
this didn't influence the test too much. To reduce possible outliers we performed
the test $100$ times. With this $N$, the total time spent by the Virtual
Machines to boot up was of $2021,15$ seconds, while with containers the same
number of start ups resulted in a total of $1.638$ seconds. Summing up, Docker
employs only the $0,08\%$ of the overall time spent for the test, underlining
how isolating the process with a set of defined policy and avoiding a
virtualized kernel boot-up saves a considerable amount of time.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[scale=0.4]{docker_vs_vmBarplotGraph}
        \caption{Barplot between Virtual Box and Docker startup times.}
        \label{chap:tests:sec:dockervsvb:img:barplot}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[scale=0.35]{docker_vs_vmBoxplotGraph}
        \caption{Boxplot between Virtual Box and Docker with values normalized 
in range $0-1$.}
        \label{chap:tests:sec:dockervsvb:img:boxplot}
    \end{subfigure}
    \caption[Virtual Box vs Docker start up comparison]{In these graphs it is
      possible to see how the total time to start up a virtual machine through
      Virtual Box is very high compared to the time required to the overall
      Docker containers to boot. A single VirtualBox instance requires on
      average 20,211 seconds to boot up, while a container requires, on average,
      only 0.016 seconds. The main reason behind this difference, as already
      explained in other parts of this thesis, it is caused by the guest's
      kernel that has to initialized itself, before starting any program. On
      Docker, instead, the kernel is the same of the host, so the only
      operations needed are to isolate the program from the rest of the OS and
      add it to the process list.}
    \label{chap:tests:sec:dockervsvb:subimg:plots}
\end{figure}


\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c}
                     & \textbf{Standard deviation (seconds)} & \textbf{Average start up time (seconds)} \\ \hline
\textbf{Docker}      & 0.009620874                           & 0,016                                    \\
\textbf{Virtual Box} & 0.755                                 & 20,211                                 
\end{tabular}%
}
\caption[Docker vs Virtual Box start up times comparison]{Docker vs Virtual Box
  start up times comparison. Is possible to see how the overall time required to
  start $100$ containers is less than the time to start one traditional virtual
  machine. The overall time required by the traditional virtualization system is
  approximately of 33 minutes, witch gives a significant insight of how much
  time is required to create and boot a virtualized environment.}
\label{my-label}
\end{table}


% The overall time required by the traitional virtualization system is
% approximately of 33 minutes, witch gives a significant insight of how much time
% is required to create and boot a virtualized enviroment.


\subsection{SFC chain deploy measurement}

The previous test make us see how much Docker is faster than a traditional
virtual machine. The test, though, was performed on a single node, interacting
directly with Docker and not through the orchestrator we used in the thesis
development, Kubernetes. The following measurement will allow us to see the time
required in order to deploy an SFC of different elements in a Kubernetes
environment. The deployment consists in a Pod inside a Deployment definition
with a related Service, to simulate a real VNF. After the Pod startup, a request
is made to a backend designed to count the chain elements that have successfully
boot-up returning, eventually, the overall start up timing.

% TODO: missing YAML definition: [language=YAML]
%https://tex.stackexchan
%ge.com/questions/152829/how-can-i-highlight-yaml-code-in
%- a-pretty-way-with-listings
\lstinputlisting[caption={Kubernetes YAML configuration that has been used to
    simulate a single VNF deployment}, captionpos=b]{res/code/sfc_test.yaml}

\vspace{0.5cm}

\noindent As is possible to see from the YAML definition, the test involved, for
every launch (that included and $N = 100$, for a total for 1000 tests), the 
download of the container image from the Docker hub repositories. This, while 
it forced every node to delete and download the image for each test, it ensured 
consistent measurements in cases where the selected node for the deployment did 
not have the image already pulled. In addition to that, another consideration 
needs to be taken into account: when the chain consists of two or more 
elements, the container image which is used for the container deployment is the 
same for every VNF involved, thus reducing the start up time for the subsequent 
pods that reside in nodes that have already deployed one.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{sfc_startupBarplotGraph}
  \caption[SFC start-up time barplot]{SFC start-up time barplot. It is
    noticeable how there seems to be a linear correlation between the SFC
    startup time and the SFC length, even though for the chain with eight
    elements it is possible to see that the overall deploy time is more compared
    to chains with nine and ten elements.}
  \label{chap:tests:sec:sfclength:img:barplot}
\end{figure}

\paragraph*{Kubernetes specifications}
A Kubernetes installation was performed in order to carry out the test
described above. All the machines involved (master node and minions) had the
same specifications i.e., 4GB of RAM, 2 vCPU and 10GB of SSD storage.
Nonetheless the constrained resources, absolutely far from a real Kubernetes
production environment, it is possible to notice how, even with a chain of ten
elements, the average time required to deploy an SFC is equal, if not smaller,
of the average time required to deploy a Virtual Box machine on an instance 
with 32GB of RAM, 8 vCPU and 50GB of SSD storage.

% TODO is it worth to keep it as a long table?
\begin{longtable}[c]{c|c|c}
\textbf{SFC length} & \textbf{Stamdard deviation (seconds)} & \textbf{Average 
deploy time (seconds)} \\ \hline
\endhead
%
\textbf{1}          & 2.560102                   & 13,291                     \\
\textbf{2}          & 0.6301444                  & 14,207                     \\
\textbf{3}          & 1.14903                    & 15,345                     \\
\textbf{4}          & 3.592179                   & 16,453                     \\
\textbf{5}          & 0.9728132                  & 16,634                     \\
\textbf{6}          & 1.103802                   & 17,418                     \\
\textbf{7}          & 1.561451                   & 18,815                     \\
\textbf{8}          & 0.7698537                  & 20,856                     \\
\textbf{9}          & 1.889197                   & 20,187                     \\
\textbf{10}         & 1.709227                   & 20,476                     \\
\caption[SFC start up time]{Table of the results. It is possible to see how the
  time required for the SFC to go up increases with the length, reaching an
  average of 20 seconds for chain of eight, nine and ten elements.}
\label{chap:tests:sec:sfclength:tab:sfcdata}\\
\end{longtable}


\section{Final considerations}

In these tests is possible to see one critical aspect concerning an architecture
that follows the containerization approach: great flexibility in terms of
deployments. This possibility to deploy services in the form of SFCs and VNFs in
a very small amount of time is one of the key points required in the new 5G
architecture, that the tests above demonstrated this project is able to adhere.
Finally, the ability to rapidly redeploy software means smooth and easier
software development lifecycle, where developers have the possibility to
automatically push ``fresh'' software to production, letting the MANO and thus
the container orchestrator handling all the difficult and delicate tasks to put
software in a production environment, eliminating the possibility of human
errors and assuring longer uptimes.

\section{System performances}

\subsection{Test system}
All the machine used for testing purposes are Openstack virtual machines, with
the following specifications:
\begin{itemize}
\item 1 machine with 32GB of RAM, 8vCPU and solid state storage, on which the
receiver of packets is running;
\item 4 machines with 16GB of RAM, 6vCPU and solid state storage, that form a
Kubernetes cluster on which SFC infrastructure is deployed;
\item 1 machine with 4GB of RAM, 2vCPU and solid state storage, on which
scripts to send packets run.
\end{itemize}

\subsection{RTT measurement}
Regarding the delivery performance, a metric to take in account is the
\emph{round trip time}(RTT), that is the time interval between the instant in
which a packet is sent and the response is received. To test it there was
developed a script that sends a packet with 8B of payload and waits for the
correspondent response, recording the instants (expressed as \emph{epoch}) of
send and receive. The sake of this test is to compare the RTT measured without
any SFC (only sending the packet to the receiver waiting for the response) and
making packets pass through different length SFCs, in particular with 1, 2, 3,
4, 5, and 10 hops. Packets traverse the SFC on both sides and the results are
collected after 100 tests and the SF only prints the packet received and the
total number of packets that have passed through it.

\begin{table}[H]
\centering
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{SFC length} & \textbf{Mean RTT (milliseconds)} \\ \midrule
len. 0              & 0.0009668279      \\
len. 1              & 0.0555884027      \\
len. 2              & 0.0702438903      \\
len. 3              & 0.0888472247      \\
len. 4              & 0.1045638967      \\
len. 5              & 0.1239343858      \\
len. 10             & 0.2135494161      \\ \bottomrule
\end{tabular}
\caption{Mean RTT measured with different SFC length}
\label{chap:tests:sec:rtt:tab:meanrtt}
\end{table}

Not surprisingly, as depicted in the Table~\ref{chap:tests:sec:rtt:tab:meanrtt},
the RTT increase with the number of elements of the chain. There is a big
difference between the mean RTT of the configuration without any chain and the
one with an SFC with a single link: this outcome has different explanation. As
explained in \todo{Add reference to implementation} the chain has always at
least 3 hops (ingress, SF, egress), so the packet must be elaborated by 3
different running software. Another explanation lays into how the code was
developed: it was not developed as a production ready product, but for a
feasibility test, so connections and transmissions can be enhanced. Finally, SFs
are meant to perform some modifications to the packet, so, unavoidably, some
time is wasted. 

\begin{figure}[H]
  %\centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[scale=0.4]{meanrtt}
        \caption{Mean RTT calculate using SFCs of different lengths}
        \label{chap:tests:sec:rtt:img:meanstt}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[scale=0.4]{differencenosfc}
        \caption{Time increase using SFCs of different lengths}
        \label{chap:tests:sec:rtt:img:differencertt}
    \end{subfigure}
\end{figure}

The results are better represented in
Figure~\ref{chap:tests:sec:rtt:img:meanstt}. In that graph we can easier see
the difference among the chains with different lengths and the transmission that
does not use any SFC. Looking at the bars representing the SFC composed by 1,
2, 3, 4 and 5 links we can see a pretty linear increase of the RTT. This trend
strongly depends on the nature of the SF involved in the SFC, in that case the
linear increase is highlighted due to the use of the same function. In
Figure~\ref{chap:tests:sec:rtt:img:differencertt} is depicted the difference
between the RTT measured with a normal packet transmission and using the chains
with different size, that represent the additional time needed for each
transmission. Even in that case we can see that some time is wasted against a
normal transmission and that this delta of time depends on the number of SFC
involved in the chain.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{differenceonehop}
  \caption{Time difference between SFCs that have a length difference of one SF}
  \label{chap:tests:sec:rtt:img:differenceonehop}
\end{figure}

In Figure~\ref{chap:tests:sec:rtt:img:differenceonehop} is represented the
difference in terms of RTT among chains that have a length difference of one
link. The first bar represent the comparison between an SFC composed by only
one element and the direct transmission to the receiver: that explain why that
value is higher than the others. Other differences among SFC, instead, are
quite similar.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{SFC length} & \textbf{Jitter (Milliseconds)} \\ \midrule
len. 0              & 0.0001074767    \\
len. 1              & 0.0028695822    \\
len. 2              & 0.0032037902    \\
len. 3              & 0.0031961584    \\
len. 4              & 0.0027278513    \\
len. 5              & 0.0036802184    \\
len. 10             & 0.0063434506    \\ \bottomrule
\end{tabular}
\caption{Jitter for different SFC lengths}
\label{chap:tests:sec:rtt:tab:jitter}
\end{table}

Table~\ref{chap:tests:sec:rtt:tab:jitter} represent the jitter calculated for
different SFC length, based on values of RTT measured in the previous test.
Jitter is the variation in the delay of received packet, and gives a measure of
the congestion of the network. As we can see values are really low, even if the
lowest result is always the one calculated without the chain.